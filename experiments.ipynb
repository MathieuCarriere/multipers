{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle as pck\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from gudhi.point_cloud.timedelay import TimeDelayEmbedding\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from multipers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the dataset! They can be obtained there: https://www.cs.ucr.edu/~eamonn/time_series_data_2018/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"GunPoint\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define all hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time delay embedding\n",
    "dimension = 3\n",
    "delay     = 1\n",
    "skip      = 1\n",
    "\n",
    "# vineyards\n",
    "nlines = 200\n",
    "noise  = .2\n",
    "\n",
    "# DTM parameters\n",
    "m = .1\n",
    "p = 1\n",
    "\n",
    "# image parameters\n",
    "res = 50\n",
    "\n",
    "# ML parameters\n",
    "split      = 100\n",
    "classifier = XGBClassifier(random_state=101)\n",
    "cv         = 5\n",
    "\n",
    "# plot\n",
    "visu = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path       = \"./\" + dataset + \"/\"\n",
    "list_filts = [\"Alpha-DTM-0\", \"Alpha-DTM-1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data sets, and impute the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = np.array(pd.read_csv(path + dataset + \"_TRAIN.tsv\", sep=\"\\t\", header=None))\n",
    "X2 = np.array(pd.read_csv(path + dataset + \"_TEST.tsv\",  sep=\"\\t\", header=None))\n",
    "X = np.vstack([X1, X2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "L, TS = X[:,0], X[:,1:]\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy=\"mean\")\n",
    "TS = imp.fit_transform(TS)\n",
    "tde = TimeDelayEmbedding(dim=dimension, delay=delay, skip=skip)\n",
    "nts = len(TS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decompositions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute maximal pairwise distance for Alpha complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = []\n",
    "for tsidx in range(0,30):\n",
    "    X = tde(TS[tsidx,:])\n",
    "    ds.append(pairwise_distances(X).flatten())\n",
    "allds = np.concatenate(ds)\n",
    "maxd = np.max(allds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute bounding rectangle for multiparameter persistence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mxf, Mxf, myf, Myf = np.inf, -np.inf, np.inf, -np.inf\n",
    "\n",
    "for tsidx in range(0, nts):\n",
    "\n",
    "    # Compute min and max of first filtration (Alpha)\n",
    "    X = tde(TS[tsidx,:])\n",
    "    st = gd.AlphaComplex(points=X).create_simplex_tree(max_alpha_square=maxd)\n",
    "    st.persistence()\n",
    "    fs = [f for (s,f) in st.get_filtration()]\n",
    "    mxf, Mxf = min(mxf, min(fs)), max(Mxf, max(fs))\n",
    "    \n",
    "    # Compute min and max of second filtration (lower-star on DTM)\n",
    "    density = DTM(X, X, m)\n",
    "    myf, Myf = min(myf, min(density)), max(Myf, max(density))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compte all multipersistence decompositions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldgms0, mdgms0 = [], []\n",
    "ldgms1, mdgms1 = [], []\n",
    "count = 0\n",
    "\n",
    "for tsidx in range(0, nts):\n",
    "\n",
    "    # Compute time delay embedding and DTM density\n",
    "    X = tde(TS[tsidx,:])\n",
    "    density = np.squeeze(DTM(X, X, m))\n",
    "    \n",
    "    # Create Alpha complex\n",
    "    dcomplex = gd.AlphaComplex(points=X)\n",
    "    st = dcomplex.create_simplex_tree(max_alpha_square=maxd)\n",
    "\n",
    "    # Use first barycentric subdivision to turn Alpha into a lower-star\n",
    "    list_splxs = []\n",
    "    st2 = gd.SimplexTree()\n",
    "    for (s,_) in st.get_filtration():\n",
    "        st2.insert(s, max([density[v] for v in s]))\n",
    "        list_splxs.append((s, max([density[v] for v in s])))\n",
    "    bary1 = barycentric_subdivision(st, use_sqrt=False)\n",
    "    bary2 = barycentric_subdivision(st2, list_splx=list_splxs)\n",
    "\n",
    "    # Write inputs for vineyards algorithm\n",
    "    cname, fname = path + \"complex\" + str(count) + \".txt\", path + \"filtrations\" + str(count) + \".txt\"\n",
    "    complexfile, filtfile = open(cname, \"w\"), open(fname, \"w\")\n",
    "    for (s,f) in bary1.get_filtration():\n",
    "        for v in s:\n",
    "            complexfile.write(str(v) + \" \")\n",
    "        complexfile.write(\"\\n\")\n",
    "        if len(s) == 1:\n",
    "            filtfile.write(str(f) + \" \" + str(bary2.filtration(s)) + \"\\n\")\n",
    "    complexfile.close()\n",
    "    filtfile.close()\n",
    "\n",
    "    # Compute the vineyards\n",
    "    mdg0, lines0, _, _ = sublevelsets_multipersistence(\n",
    "        \"./bin/vine\", cname, fname, homology=0, num_lines=nlines, corner=\"dg\", extended=False, essential=False,\n",
    "        epsilon=1e-10, min_bars=1, noise=noise, parallel=False, nproc=None, visu=visu, plot_per_bar=False, \n",
    "        bnds_filt=[mxf,Mxf,myf,Myf], bnds_visu=[mxf,Mxf,myf,Myf])\n",
    "    mdg1, lines1, _, _ = sublevelsets_multipersistence(\n",
    "        \"./bin/vine\", cname, fname, homology=1, num_lines=nlines, corner=\"dg\", extended=False, essential=False,\n",
    "        epsilon=1e-10, min_bars=1, noise=noise, parallel=False, nproc=None, visu=visu, plot_per_bar=False, \n",
    "        bnds_filt=[mxf,Mxf,myf,Myf], bnds_visu=[mxf,Mxf,myf,Myf])\n",
    "\n",
    "    mdgms0.append(mdg0)\n",
    "    mdgms1.append(mdg1)\n",
    "    count += 1\n",
    "\n",
    "    os.system(\"rm \" + cname + \"*\")\n",
    "    os.system(\"rm \" + fname + \"*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(path + \"labels.txt\", L[:nts])\n",
    "np.save(path + \"lines_Alpha-DTM-0\", lines0)\n",
    "np.save(path + \"lines_Alpha-DTM-1\", lines1)\n",
    "np.save(path + \"bnds_Alpha-DTM-0\", np.array([mxf,Mxf,myf,Myf]))\n",
    "np.save(path + \"bnds_Alpha-DTM-1\", np.array([mxf,Mxf,myf,Myf]))\n",
    "pickle.dump(mdgms0, open(path + \"mdgms_Alpha-DTM-0.pkl\", \"wb\"))\n",
    "pickle.dump(mdgms1, open(path + \"mdgms_Alpha-DTM-1.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_lines, list_bnds, list_mdgms, list_delta = [], [], [], []\n",
    "for filtname in list_filts:\n",
    "    list_lines.append(np.load(path + \"lines_\" + filtname + \".npy\"))\n",
    "    list_bnds.append(np.load(path + \"bnds_\" + filtname + \".npy\"))\n",
    "    list_mdgms.append(pck.load(open(path + \"mdgms_\" + filtname + \".pkl\", \"rb\")))\n",
    "    delta = np.abs(lines[0,0]-lines[1,0]) if lines[0,0] != lines[1,0] else np.abs(lines[2,2]-lines[1,2])\n",
    "    list_delta.append(delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Multiparameter Persistence Images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filtidx in range(len(list_filts)):\n",
    "    \n",
    "    bnds  = list_bnds [filtidx]\n",
    "    mdgms = list_mdgms[filtidx]\n",
    "\n",
    "    MPI = [multipersistence_image(mdg, bnds, resolution=[res,res], return_raw=True) for mdg in mdgms]\n",
    "    pck.dump(MPI, open(path + \"mpi_\" + str(res) + \"_\" + filtname + \".pkl\", \"wb\"))\n",
    "    print(\"MPI done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Multiparameter Persistence Landscapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filtidx in range(len(list_filts)):\n",
    "    \n",
    "    bnds  = list_bnds [filtidx]\n",
    "    mdgms = list_mdgms[filtidx]\n",
    "    delta = list_delta[filtidx]\n",
    "    \n",
    "    MLS = [multipersistence_landscape(mdg,bnds,delta,resolution=[res,res],k=5,return_raw=True) for mdg in mdgms]\n",
    "    pck.dump(MLS, open(path + \"mls_\" + str(res) + \"_\" + filtname + \".pkl\", \"wb\"))\n",
    "    print(\"MLS done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Multiparameter Persistence Kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filtidx in range(len(list_filts)):\n",
    "    \n",
    "    lines = list_lines[filtidx]\n",
    "    bnds  = list_bnds [filtidx]\n",
    "    mdgms = list_mdgms[filtidx]\n",
    "    \n",
    "    MK  = [extract_diagrams(mdg, bnds, lines) for mdg in mdgms]\n",
    "    sw = sktda.SlicedWassersteinDistance(num_directions=10)\n",
    "    M = multipersistence_kernel(MK, MK, lines, sw, lambda x: 1, same=True, return_raw=False, power=0)\n",
    "    pck.dump(M, open(path + \"mk_\" + filtname + \".pkl\", \"wb\"))\n",
    "    print(\"MK done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect the diagonal barcodes for 1D persistence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fibs = []\n",
    "for filtidx in range(len(list_filts)):\n",
    "    \n",
    "    lines = list_lines[filtidx]\n",
    "    bnds  = list_bnds [filtidx]\n",
    "    mdgms = list_mdgms[filtidx]\n",
    "    \n",
    "    ldgms = []\n",
    "    for decomposition in mdgms:\n",
    "        if len(decomposition) > 0:\n",
    "            mdgm = np.vstack(decomposition)\n",
    "            al = int(len(lines)/2)\n",
    "            for a in range(len(lines)):\n",
    "                if lines[a,0] == min(lines[:,0]) and lines[a,1] == min(lines[:,1]):\n",
    "                    al = a\n",
    "                    break\n",
    "            dg = []\n",
    "            idxs = np.argwhere(mdgm[:,4] == al)[:,0]\n",
    "            if len(idxs) > 0:\n",
    "                dg.append(mdgm[idxs][:,:4])\n",
    "            if len(dg) > 0:\n",
    "                dg = np.vstack(dg)\n",
    "                dg = intersect_boundaries(dg, bnds)\n",
    "                if len(dg) > 0:\n",
    "                    xalpha, yalpha, xAlpha, yAlpha = lines[al,0], lines[al,1], lines[al,2], lines[al,3]\n",
    "                    pt = np.array([[xalpha, yalpha]])\n",
    "                    st, ed = dg[:,[0,2]], dg[:,[1,3]]\n",
    "                    dgm = np.hstack([ np.linalg.norm(st-pt, axis=1)[:,np.newaxis], \n",
    "                                      np.linalg.norm(ed-pt, axis=1)[:,np.newaxis] ])\n",
    "                else:\n",
    "                    dgm = np.array([[.5*(bnds[0]+bnds[1]), .5*(bnds[2]+bnds[3])]])\n",
    "            else:\n",
    "                dgm = np.array([[.5*(bnds[0]+bnds[1]), .5*(bnds[2]+bnds[3])]])\n",
    "        else:\n",
    "            dgm = np.array([[.5*(bnds[0]+bnds[1]), .5*(bnds[2]+bnds[3])]])\n",
    "        ldgms.append(dgm)\n",
    "    fibs.append(ldgms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute persistence landscapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filtidx in range(len(list_filts)):\n",
    "    \n",
    "    fib = fibs[filtidx]\n",
    "    \n",
    "    ldgmsLS = [dg for dg in fib]\n",
    "    L = sktda.Landscape(num_landscapes=5, resolution=2500, sample_range=[np.nan, np.nan]).fit_transform(ldgmsLS)\n",
    "    pck.dump(L, open(path + \"ls_\" + filtname + \".pkl\", \"wb\"))\n",
    "    print(\"LS done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute persistence images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filtidx in range(len(list_filts)):\n",
    "    \n",
    "    fib = fibs[filtidx]\n",
    "    \n",
    "    ldgmsPI = [dg for dg in fib]\n",
    "    ldgmsPI = [np.hstack([dgm[:,0:1], dgm[:,1:2]-dgm[:,0:1]]) for dgm in ldgmsPI]\n",
    "    PXs, PYs = np.vstack([dgm[:,0:1] for dgm in ldgms]), np.vstack([dgm[:,1:2] for dgm in ldgms])\n",
    "    bnds = [PXs.min(), PXs.max(), PYs.min(), PYs.max()]\n",
    "    PI = [persistence_image(dgm=dgm, bnds=bnds, return_raw=True) for dgm in ldgmsPI]\n",
    "    pck.dump(PI, open(path + \"pi_\" + str(resolution) + \"_\" + filtname + \".pkl\", \"wb\"))\n",
    "    print(\"PI done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.loadtxt(path + \"labels.txt\", dtype=float)\n",
    "labels = np.array([int(l) for l in labels])\n",
    "npoints = len(labels)\n",
    "train_index, test_index = np.arange(split), np.arange(split, npoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classify the Multiparameter Persistence Images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xmpi = [pck.load(open(path + \"mpi_\" + str(resolution)  + \"_\" + filt + \".pkl\", \"rb\")) for filt in list_filts]\n",
    "params_mpi = {\n",
    "    \"mpi__bdw\":     [1e-2, 1e-1, 1, 1e1, 1e2],\n",
    "    \"mpi__power\":   [0, 1],\n",
    "    \"mpi__step\":    [1, 5],\n",
    "    \"clf\":          [classifier],\n",
    "}\n",
    "pipe_mpi = Pipeline([(\"mpi\", MultiPersistenceImageWrapper()), (\"clf\", classifier)])\n",
    "X_train  = [[Xmpi[nf][n] for nf in range(len(Xmpi))] for n in train_index]\n",
    "Xtest    = [[Xmpi[nf][n] for nf in range(len(Xmpi))] for n in test_index]\n",
    "y_train, y_test = labels[train_index], labels[test_index]\n",
    "model = GridSearchCV(estimator=pipe_mpi, param_grid=params_mpi, cv=cv)\n",
    "model.fit(X_train, y_train)\n",
    "score = model.score(X_test, y_test)\n",
    "print(\"MP-I score = \" + str(score))\n",
    "pck.dump([model.best_params_, model.cv_results_, score], \n",
    "         open(path + \"modelMPI_CV\" + str(cv) + \"_\" + filtname + \".pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classify the Multiparameter Persistence Landscapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xmls = [pck.load(open(path + \"mls_\" + str(resolution)  + \"_\" + filt + \".pkl\", \"rb\")) for filt in list_filts]\n",
    "params_mls = {\n",
    "    \"mls__power\":   [0, 1],\n",
    "    \"mls__step\":    [1, 5],\n",
    "    \"mls__k\":       [5],\n",
    "    \"clf\":          [classifier],\n",
    "}\n",
    "pipe_mls = Pipeline([(\"mls\", MultiPersistenceLandscapeWrapper()), (\"clf\", classifier)])\n",
    "X_train  = [[Xmls[nf][n] for nf in range(len(Xmls))] for n in train_index]\n",
    "X_test   =[[Xmls[nf][n] for nf in range(len(Xmls))] for n in test_index]\n",
    "y_train, y_test = labels[train_index], labels[test_index]\n",
    "model = GridSearchCV(estimator=pipe_mls, param_grid=params_mls, cv=cv)\n",
    "model.fit(X_train, y_train)\n",
    "score = model.score(X_test, y_test)\n",
    "print(\"MP-L score = \" + str(score))\n",
    "pck.dump([model.best_params_, model.cv_results_, score], \n",
    "         open(path + \"modelMLS_CV\" + str(cv) + \"_\" + filtname + \".pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classify the Multiparameter Persistence Kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xmk = [pck.load(open(path + \"mk_\" + filt + \".pkl\", \"rb\")) for filt in list_filts]\n",
    "Xmk = sum([  Xmk[nf] for nf in range(len(Xmk))  ])\n",
    "X_train, X_test = Xmk[train_index,:][:,train_index], Xmk[test_index,:][:,train_index]\n",
    "y_train, y_test = labels[train_index], labels[test_index]\n",
    "model = SVC(kernel=\"precomputed\")\n",
    "model.fit(X_train, y_train)\n",
    "score = model.score(X_test, y_test)\n",
    "print(\"MP-K score = \" + str(score))\n",
    "pck.dump([0, 0, score], open(path + \"modelMK_SWK10_CV1_\" + filtname + \".pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classify the persistence landscapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xls = [pck.load(open(path + \"ls_\" + filt + \".pkl\", \"rb\")) for filt in list_filts]\n",
    "Xls = np.hstack(Xls)\n",
    "params_ls = {\n",
    "    \"sbs__step\":    [1, 25],\n",
    "    \"clf\":          [classifier],\n",
    "}\n",
    "pipe_ls = Pipeline([(\"sbs\", SubsampleWrapper()), (\"clf\", classifier)])\n",
    "X_train, X_test = Xls[train_index,:], Xls[test_index,:]\n",
    "y_train, y_test = labels[train_index], labels[test_index]\n",
    "model = GridSearchCV(estimator=pipe_ls, param_grid=params_ls, cv=cv)\n",
    "model.fit(X_train, y_train)\n",
    "score = model.score(X_test, y_test)\n",
    "print(\"P-L score = \" + str(score))\n",
    "pck.dump([model.best_params_, model.cv_results_, score], \n",
    "         open(path + \"modelLS_CV\" + str(cv) + \"_\" + filtname + \".pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classify the persistence images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xpi = [pck.load(open(path + \"pi_\" + str(resolution)  + \"_\" + filt + \".pkl\", \"rb\")) for filt in list_filts]\n",
    "params_pi = {\n",
    "    \"pi__bdw\":     [1e-2, 1e-1, 1, 1e1, 1e2],\n",
    "    \"pi__power\":   [0, 1],\n",
    "    \"pi__step\":    [1, 5],\n",
    "    \"clf\":         [classifier],\n",
    "}\n",
    "pipe_pi = Pipeline([(\"pi\", PersistenceImageWrapper()), (\"clf\", classifier)])\n",
    "X_train = [[Xpi[nf][n] for nf in range(len(Xpi))] for n in train_index] \n",
    "X_test  = [[Xpi[nf][n] for nf in range(len(Xpi))] for n in test_index]\n",
    "y_train, y_test = labels[train_index], labels[test_index]\n",
    "model = GridSearchCV(estimator=pipe_pi, param_grid=params_pi, cv=cv)\n",
    "model.fit(X_train, y_train)\n",
    "score = model.score(X_test, y_test)\n",
    "print(\"P-I score = \" + str(score))\n",
    "pck.dump([model.best_params_, model.cv_results_, score], \n",
    "         open(path + \"modelPI_CV\" + str(cv) + \"_\" + filtname + \".pkl\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
